<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="environment_sizing" audience="expert">
    <title>Environment sizing guidelines</title>
    <shortdesc>
        To size an environment precisely, you must understand the factors such as harvest frequency, complexity of the source, and use case scenarios that drive application use and action execution.
    </shortdesc>
    <prolog>
        <metadata>
            <keywords>
                <indexterm>sizing<index-see>deployment</index-see></indexterm>
            </keywords>
        </metadata>
    </prolog>
    <conbody>
        <p>The general design guidelines for IBM StoredIQ are as follows:</p>
        <ul>
            <li>For data servers of the type DataServer - Classic:
                <ul>
                    <li>One data server for up to 30 TBs of data (which can vary based on the number of volumes, objectsper volume, and object types).</li>
                    <li>p to 500 volumes per data server. 
                        <note type="tip">When you're sizing an environment that includes Sharepoint data sources, keep in mind thatvolumes must be defined at Sharepoint site collection level, not the Sharepoint server level.</note>
                    </li>
                    <li>Up to 150 million objects per data server.</li>
                </ul>
            </li>
            <li>One gateway per 50 data servers.</li>
            <li>One application server.</li>
            <li>NFS is slightly faster than CIFS for metadata only, but assume CIFS/NFS even for this exercise.</li>
            <li>Full-content processing of file (for example, .ZIP, .RAR, .GZ) and email archive (.PST, .NSF, .EMX)processing are slower as items must be extracted from the archives. If there is a significant number ofthese files in the file system and they are not excluded from content processing, the full-content processing rate can be too high. Until you have an initial index of the file system, you do not know howto weigh full-content processing of archives.</li>
            <li>An object/time metric is appropriate for metadata only NOT computing a hash, membership in theNational Institute of Standards and Technology (NIST) or enumerating objects that are contained inarchives. Converting it to a bytes/time metric is a function of the average object size and might varytremendously. An average object size of 250 KB was used for the metric that is provided earlier.</li>
            <li>A bytes/time metric is appropriate for metadata-only computing a hash and full-content processing.The object per second rate can vary tremendously depending on the object type and sizes encountered.For example, processing an email or file archive is much more expensive than a PDF document.</li>
            <li>Metadata-only not computing a hash, membership in the NIST list, or enumerating objects that arecontained in archives is requesting only the file-attribute information from the NAS. Individual files arenot opened and read. The processing rate is high, but that does not translate into a large amount of datathat traverses a network between the NAS and data server. The bytes/time rate does not translate intobytes served by the NAS and sent over the network.</li>
            <li>Metadata-only computing a hash, membership in the NIST list, or enumerating objects that arecontained in archives opens and reads the contents of each file. The content of all requested filestraverses the network between the NAS and data server. The maximum load that the data server canplace on a NAS is metadata-only processing. It requires all file content to be read to compute a hash orenumerate objects that are contained in archives. The bytes/time rate translates into bytes served upby the NAS and network traffic that must be considered.</li>
            <li>Full-content processing opens and reads the contents of each file to extract all text. The content of allrequested files traverses the network between the NAS and data server. The processing time toenumerate archives, extract text, index words, and extract entities on the data server reduces the ratethat data is requested from a NAS compared to metadata-only with full hash. The bytes/time ratetranslates into bytes served up by the NAS and network traffic that must be considered.</li>
            <li>The interrogator process count on the data server for "metadata only not reading all content indexing" isset to eight for optimal performance.</li>
            <li>The interrogator process count for all other processing that involves reading all content default settingis four per data server.</li>
            <li>The interrogator count can be viewed as the number of client connections that are made to a datasource actively requesting data. It is important for capacity planning for the data source.</li>
            <li>The data servers are assumed to be "network close" to the NAS data sources. Network latency under 10ms with at least 1000 Mbps bandwidth is assumed (connected through local area network). The dataservers need a low latency high-bandwidth connection to a NAS data source for acceptable indexingperformance.</li>
            <li>The gateway and application stack can be located remotely from the data servers. Network connectionswith latency greater than 10 ms and bandwidth of at least 2+ Mbps are acceptable.</li>
        </ul>
        
        <section>
            <title>VMware requirements</title>
            <ul>
                <li>VMware vSphere v5.0 and fix packs or v6.0 and fix packs.</li>
                <li>VMware ESXi v5.0 and fix packs, v6.0 and fix packs, or v6.5 and fix packs.</li>
                <li>VMware virtual machine hardware version 8.0 or later. For more information, see the VMware productdocumentation.</li>
                <li>The appropriate VMware license to enable the required processor cores and memory for the virtualmachine.</li>
            </ul>
        </section>
    </conbody>
</concept>
